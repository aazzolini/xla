#ifndef XLA_TORCH_XLA_CSRC_COMPUTATION_H_
#define XLA_TORCH_XLA_CSRC_COMPUTATION_H_

#include <torch/csrc/lazy/backend/lowering_context.h>
#include <torch/csrc/lazy/core/hash.h>

#include "torch_xla/csrc/runtime/computation_client.h"

namespace torch_xla {

// There are 5 different Computation class being used here
// 1. torch::lazy::Computation represent a general computation from LTC
// perspective.
// 2. torch_xla::Computation inherits torch::lazy::Computation and represent a
// torch/xla computation. It wraps a runtime::ComputationClient::Computation.
// 3. runtime::ComputationClient::Computation represent a computation from the
// ComputationClient perspective. It wraps a xla::XlaComputation and a vector of
// device.
// 4. xla::XlaComputation represent a xla computation, it is generated by the
// xla compiler.
// 5. xla::XrtComputationClient::XrtComputation and
// xla::PjRtComputationClient::PjRtComputation which inherits from
// runtime::ComputationClient::Computation and contains a handle to represent
// the compiled program.

// torch_xla::Computation is being used for 3 different purpose.
// 1. To represent a xla computation build by xla_op_builder, in which case we
// would need the name and hash. Computation would be a wrapper around a
// runtime::ComputationClient::Computation.
// runtime::ComputationClient::Computation::devices_ would be empty.
// 2. To represent a computation built by syncTensor and needs to be compiled.
// In this case hash_ and name_ are not required. Computation would be a wrapper
// around a runtime::ComputationClient::Computation.
// 3. To represent a computation that is already compiled. In this case name_
// and hash_ are not required. Computation will be a wrapper around
// xla::XrtComputationClient::XrtComputation or
// xla::PjRtComputationClient::PjRtComputation.
// It is not ideal to use same class for 3 different purposes but this is the
// path took by upstream ltc.
// using Computation = runtime::ComputationClient::Computation;

using Computation = runtime::ComputationClient::Computation;
using ComputationPtr = std::shared_ptr<runtime::ComputationClient::Computation>;

std::vector<torch::lazy::ComputationPtr> WrapClientComputation(
    std::vector<std::shared_ptr<runtime::ComputationClient::Computation>>
        computations);

std::shared_ptr<runtime::ComputationClient::Computation>
UnwrapClientComputation(torch::lazy::ComputationPtr computation);

}  // namespace torch_xla

#endif  // XLA_TORCH_XLA_CSRC_COMPUTATION_H_

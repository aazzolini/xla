diff --git a/xla/service/layout_assignment.cc b/xla/service/layout_assignment.cc
index 5b90db59d..c6cf75e30 100644
--- a/xla/service/layout_assignment.cc
+++ b/xla/service/layout_assignment.cc
@@ -148,16 +148,24 @@ bool BufferLayoutConstraint::UpdateLayout(int64_t priority,
 
 OperandLayoutConstraint::OperandLayoutConstraint(
     const ShapeLayout& shape_layout, const HloInstruction* instruction,
-    int64_t operand_no, bool mandatory, bool dfs, int64_t priority)
+    int64_t operand_no, bool mandatory, bool dfs, int64_t priority,
+    bool is_collective_instruction)
     : LayoutConstraint(mandatory, dfs, priority),
       instruction_(instruction),
       operand_no_(operand_no) {
   CHECK(shape_layout.LayoutIsSet());
-  CHECK(ShapeUtil::Compatible(shape_layout.shape(),
-                              instruction->operand(operand_no)->shape()))
-      << shape_layout.shape() << " is not compatible with "
-      << instruction->operand(operand_no)->shape() << " (for operand "
-      << operand_no << " of instruction " << instruction->ToString() << ")";
+  bool constrain_result =
+      is_collective_instruction
+          ? ShapeUtil::SameRank(shape_layout.shape(),
+                                instruction->operand(operand_no)->shape())
+          : ShapeUtil::CompatibleIgnoringElementType(
+                shape_layout.shape(),
+                instruction->operand(operand_no)->shape());
+  CHECK(constrain_result) << shape_layout.shape() << " is not compatible with "
+                          << instruction->operand(operand_no)->shape()
+                          << " (for operand " << operand_no
+                          << " of instruction " << instruction->ToString()
+                          << ")";
   shape_layout_.push_back(shape_layout);
 }
 
@@ -359,9 +367,9 @@ Status LayoutAssignment::SetOperandLayout(const Shape& shape_with_layout,
       return OkStatus();
     }
   }
-  OperandLayoutConstraint new_constraint(ShapeLayout(shape_with_layout),
-                                         instruction, operand_no, mandatory,
-                                         dfs, priority);
+  OperandLayoutConstraint new_constraint(
+      ShapeLayout(shape_with_layout), instruction, operand_no, mandatory, dfs,
+      priority, IsLayoutConstrainedCollective(instruction));
   auto op_constraint = constraints.InsertOperandLayoutConstraint(
       instruction, operand_no, new_constraint);
   PushAddedConstraints(op_constraint);
diff --git a/xla/service/layout_assignment.h b/xla/service/layout_assignment.h
index f6acd9ea8..56fb8095f 100644
--- a/xla/service/layout_assignment.h
+++ b/xla/service/layout_assignment.h
@@ -113,7 +113,8 @@ class OperandLayoutConstraint : public LayoutConstraint {
  public:
   OperandLayoutConstraint(const ShapeLayout& shape_layout,
                           const HloInstruction* instruction, int64_t operand_no,
-                          bool mandatory, bool dfs, int64_t priority);
+                          bool mandatory, bool dfs, int64_t priority,
+                          bool is_collective_instruction);
 
   const ShapeLayout& shape_layout() const { return shape_layout_[0]; }
   const HloInstruction* instruction() const { return instruction_; }
diff --git a/xla/tests/collective_ops_test.cc b/xla/tests/collective_ops_test.cc
index 150601c86..4719914e2 100644
--- a/xla/tests/collective_ops_test.cc
+++ b/xla/tests/collective_ops_test.cc
@@ -1134,6 +1134,40 @@ XLA_TEST_F(CollectiveOpsTest, ReduceScatter) {
   LiteralTestUtil::ExpectR1Equal<uint32_t>({19, 21, 23, 25}, results[1]);
 }
 
+XLA_TEST_F(CollectiveOpsTest, ReduceScatterConstrainLayout) {
+  const char* const kModuleStr = R"(
+  HloModule reduce-scatter
+    %sum (a: u32[], b: u32[]) -> u32[] {
+    %a = u32[] parameter(0)
+    %b = u32[] parameter(1)
+    ROOT %add = u32[] add(u32[] a, u32[] b)
+  }
+  ENTRY main {
+    %param = u32[16] parameter(0)
+    ROOT %rs = u32[8] reduce-scatter(u32[16] %param), replica_groups={},
+                       constrain_layout=true, to_apply=%sum, dimensions={0}
+  }
+  )";
+
+  const int64_t kNumReplicas = 2;
+  HloModuleConfig config =
+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas);
+  TF_ASSERT_OK_AND_ASSIGN(auto module,
+                          ParseAndReturnVerifiedModule(kModuleStr, config));
+
+  std::vector<uint32_t> input_vec = {
+      {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}};
+  auto input_literal = LiteralUtil::CreateR1<uint32_t>(input_vec);
+  TF_ASSERT_OK_AND_ASSIGN(
+      std::vector<Literal> results,
+      ExecuteReplicated(std::move(module), {&input_literal}, kNumReplicas,
+                        /*use_threads=*/true, /*run_hlo_passes=*/true));
+  LiteralTestUtil::ExpectR1Equal<uint32_t>({2, 4, 6, 8, 10, 12, 14, 16},
+                                           results[0]);
+  LiteralTestUtil::ExpectR1Equal<uint32_t>({18, 20, 22, 24, 26, 28, 30, 32},
+                                           results[1]);
+}
+
 XLA_TEST_F(CollectiveOpsTest, ReduceScatter_Dim1) {
   const char* const kModuleStr = R"(
   HloModule test
